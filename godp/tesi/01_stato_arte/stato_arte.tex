\chapter{Stato dell'arte}

\section{Introduzione alla Privacy Differenziale}
La privacy differenziale (DP) è un rigoroso framework matematico per la progettazione di algoritmi utilizzati per la creazione di distribuzioni di dati aggregati su dataset; gli algoritmi che rispettano la definizione di DP sono in grado di limitare l'impatto che la partecipazione di un singolo individuo ha sui risultati dell'analisi, aggiungendo una ragionevole quantità di rumore casuale, rendendo \textit{quasi} impossibile confermare o negare la presenza di un soggetto all'interno del dataset utilizzato per l'aggregazione di dati \cite{Hilton2012Survey}. Questa proprietà è cruciale per prevenire che eventuali attaccanti possano inferire informazioni sensibili su individui partecipanti al dataset, specialmente in presenza di fonti di informazione ausiliarie.

Il framework della DP nasce con l'obiettivo di sopperire alla necessità di pubblicare statistiche aggregate partendo da raccolte di dati sensibili limitando la probabilità di esporre informazioni private su singoli soggetti del database. In particolare, l'obiettivo dei meccanismi DP è quello di limitare la probabilità che sia possibile confermare o negare la presenza di un soggetto in un dataset osservando il risultato di una aggregazione sullo stesso \cite{dwork2008survey}.

Gli algoritmi DP sono specialmente utili grazie alla resistenza ad attacchi di deanonimizzazione che sfruttano informazioni da dati provenienti da altre fonti \cite{TCS-042}, caratteristica desiderabile in un contesto moderno che vede una quantità costantemente crescente di dati potenzialmente privati di singoli soggetti esposti al pubblico sia a causa di data breach \cite{experianBreachForecast} che a causa di diffusione intenzionale su social network e piattaforme simili.

Con la proliferazione di machine learning, big data e la generale condivisione pervasiva di dati, le tecniche di anonimizzazione tradizionali si sono rivelate inadeguate \cite{dwork2008survey}. Queste tecniche falliscono nel garantire privacy ai soggetti di un dataset a causa della disponibilità di dati da fonti come social network \cite{deanon-socialnet}, piattaforme di streaming \cite{deanon-netflix} e dispositivi di geo-localizzazione \cite{deanon-geodata}, permettendo a un attaccante di associare dati sensibili agli individui.

\section{Contesto storico e metodi tradizionali per la protezione della privacy}
Il concetto di privacy differenziale è stato introdotto formalmente da Cynthia Dwork \cite{10.1007/11681878_14}, dove viene formalizzata la definizione di $\varepsilon$-DP e dimostrato come l'aggiunta di rumore calibrato alla sensitività di una funzione possa proteggere la privacy individuale mantenendo un certo grado di utilità dei dati.

Prima dell'avvento della DP, l'anonimizzazione dei dati si basava su tecniche come k-anonimity, basata sulla soppressione o generalizzazione di determinati campi del dataset \cite{kanon}, e altre tecniche derivate come t-closeness e l-diversity che impongono limiti più stretti sulle distribuzioni dei valori degli attributi \cite{tClosenesslDiversity}. Queste tecniche permettono di creare distribuzioni di dati che, prese da sole, garantiscono un alto livello di privacy, richiedendo metodi sofisticati per estrarre informazioni utili; sono tuttavia più vulnerabili ad attacchi di rianonimizzazione che sfruttano informazioni ausiliarie \cite{downcodingAtk}.

Un gestore di dati che effettua pubblicazioni di statistiche su questi dati non è in grado di controllare quali informazioni aggiuntive sui partecipanti al dataset sono disponibili o verranno rese disponibili in futuro a un potenziale attaccante, la DP costituisce una soluzione ideale rispetto ai metodi tradizionali, data la particolare resistenza agli attacchi che si basano sulla disponibilità di informazioni aggiuntive.

Un'altra differenza fondamentale che la DP presenta rispetto a metodi tradizionali è la possibilità di quantificare il livello di privacy perso con una distribuzione di dati tramite un apposito parametro; questa caratteristica, supportata dalle caratteristiche di composizione della DP\cite{dpComposition}, permette di stabilire e tracciare un budget di privacy per uno specifico dataset al contrario dei metodi tradizionali sopra citati, combinando nel modo corretto i parametri di privacy.

\section{Utilizzi DP}
Il framework della DP è stato adottato in diversi ambiti, a partire dall'analisi lessicale per sistemi di suggerimenti per tastiere alla raccolta di informazioni sull'utilizzo di piattaforme web; di seguito verranno documentati alcuni esempi.

Apple utilizza la DP con modello locale per anonimizzare i dati raccolti da dispositivi iOS e macOS \cite{appledpatscale}; questi dati vengono utilizzati per migliorare le funzionalità di suggerimenti per tastiera e identificazione di siti web che causano interruzioni o altri errori nel browser Safari.

Google ha utilizzato la DP in svariati contesti, in particolare ha spesso utilizzato il framework per analisi dall'enorme mole di dati GPS ottenuti dai dispositivi con un account Google. Una di queste analisi, Community Mobility Reports \cite{googlecovid19communitymobility}, nasce per quantificare cambiamenti in pattern di mobilità durante la pandemia COVID-19: qui la DP permette di non pubblicare mai valori assoluti sulle visite.

La Audience Engagements API di Linkedin costituisce un altro interessante esempio di applicazione del framework perché si tratta di un'implementazione del modello distribuito interattivo: diversamente dagli esempi precedenti, il rilascio dei dati non avviene tutto contemporaneamente, un \textit{analista} richiede i dati di cui necessita in modo incrementale; questo sistema impone limitazioni nella quantità di dati accessibili in un dato periodo per poter garantire un certo livello di anonimizzazione \cite{rogers2020linkedinsaudienceengagementsapi}.

\section{DP e machine learning}
Negli ultimi anni, modelli di machine learning vengono sfruttati sempre più spesso in ambiti che hanno a che fare con domini sensibili come sanità \cite{mlHealthcareSOA} e finanza \cite{llmFinanceTrends}, ambiti nei quali vengono spesso trattati dataset che includono informazioni riservate sugli individui.

Per questa tipologia di applicazione del machine learning sono attualmente in fase di sviluppo diversi metodi per limitare la perdita di privacy dei soggetti inclusi nei dataset utilizzati per l'addestramento \cite{privacyTechMlFinInt} \cite{mlHealthcareFuture}, ad esempio tecniche di protezione della privacy basate su crittografia omomorfa sono state implementate in sistemi di analisi di dati per la rilevazione di pattern che distinguono schemi di riciclaggio di denaro \cite{privacyMlFinanceHomomorphicEnc}. Tecniche come quest'ultima, oltre ad essere applicabili a una gamma di applicazioni ristretta data la natura estremamente specifica degli algoritmi coinvolti, comportano scelte basate su compromessi tra preservazione della privacy e efficienza computazionale.

L'allenamento di modelli ML su dati sensibili può portare, durante il loro utilizzo, alla distribuzione involontaria di questi dati \cite{Zhang_2023mlprivacy}. Approfittando di tecniche come model inversion \cite{OWASPmlinversion:online} e membership inference \cite{OWASPmembinference:online} potenziali attaccanti in grado di sfruttare le vulnerabilità connesse presentano un rischio significativo per la privacy degli individui e per la sicurezza delle organizzazioni che gestiscono tali dati.
%%%TODO: DESCRIVI MOD ATTACCO MEMBERSHIP INFERENCE (INTERESSANTE PER DP) DA
% https://arxiv.org/abs/2103.07853
% https://arxiv.org/abs/2310.00108
% https://owasp.org/www-project-machine-learning-security-top-10/docs/ML04_2023-Membership_Inference_Attack


Per sopperire a queste vulnerabilità, sono state sviluppate diverse tecniche che incorporano la DP durante l'addestramento o durante l'inferenza sui dati.

Uno dei primi esempi di questi metodi è la tecnica \textit{Differentially Private Stochastic Gradient Descent} \cite{Abadi_2016dpsgd} che consiste in una variante dell'algoritmo SGD standard che prevede l'aggiunta di rumore accuratamente calibrato ai gradienti a ogni iterazione del processo di addestramento, questo approccio combinato con l'imposizione di un limite superiore ai valori dei gradienti permette di limitare la ; questa tecnica è usata per addestrare modelli di deep learning.

Un altro approccio a questo problema nell'ambito della classificazione è \textit{Private Aggregation of Teacher Ensembles}, tecnica che addestra un modello \textit{studente} utilizzando i dati forniti da un insieme di modelli \textit{insegnanti} addestrati su set disgiunti del dataset iniziale; l'output fornito dall'insieme di insegnanti è reso privato con l'aggiunta di rumore opportunamente calibrato \cite{papernot2018scalableprivatelearningpate}.

L'approccio di federated learning consente di allenare un modello di machine learning in maniera distribuita, senza dover condividere i dati utilizzati per l'addestramento; i client condividono il risultato dell'allenamento con un server centrale che agisce da aggregatore creando un unico modello.

Questa tecnica da sola garantisce un certo livello di privacy nei dati dato che non è necessario che l'aggregatore abbia accesso al dataset completo; le garanzie di privacy di questo modello possono tuttavia essere migliorate condividendo gli aggiornamenti dei modelli locali al server dopo averli mascherati con un algoritmo differenzialmente privato. Il rumore aggiunto assicura che la contribuzione di una singola voce del dataset non possa essere inferita facilmente \cite{peterson2019privatefederatedlearningdomain}.

\section{Strumenti per la DP}
Negli ultimi anni sono nati vari strumenti per permettere a utenti non tecnici di creare release di dati protette dalle garanzie offerte dalla DP.

Nell'ambito open source, i framework più utilizzati sono OpenDP e le Google Differential Privacy Libraries.

Uno degli strumenti più diffusi è \textit{OpenDP}, una libreria open source sviluppata dall'Harvard University, che fornisce una suite di algoritmi per l'analisi statistica, ed è basata parzialmente sulle librerie di Google. Offre implementazioni in Rust e Python così come un'interfaccia SQL per integrare la libreria con workflow esistenti \cite{opendp}.

Le librerie di Google sono un insieme di strumenti per realizzare applicazioni che includono l'utilizzo della DP; Google fornisce una libreria di elementi base in Go, Java e C++ che include le primitive per realizzare aggregazioni differenzialmente private per l'aggiunta di rumore calibrato.\\
Questi elementi costituiscono la base del framework Privacy on Beam, un framework che integra tecniche di DP all'interno di pipeline di elaborazione dati composte con Apache Beam.\\
In questo pacchetto di Google sono inclusi anche componenti per la gestione del budget della privacy e per effettuare verifiche formali sulle garanzie offerte dalla DP.

Passando a strumenti enterprise, Tumult Labs propone una soluzione per applicazioni industriali ad alta scalabilità basata su Apache Spark distribuita sotto forma di una libreria Python. Offrono soluzioni mirate al settore pubblico nel rispetto delle normative vigenti e al settore finanziario \cite{tumultanalytics}.

In questo lavoro di tesi si mira a creare uno strumento basato su Privacy on Beam con l'obiettivo di sviluppare una soluzione che consenta la creazione di release di dati differenzialmente private di piccoli-medi dataset tramite una semplice interfaccia da linea di comando e file di configurazione. Questo strumento è pensato per essere adatto a progetti di piccola entità, ad esempio per semplificare la creazione di release di dati per gruppi di ricerca, piccoli media outlet o interne a un'azienda.







