# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Implements an (epsilon,delta)-DP test using hockey-stick divergence.

Specifically, tries to estimate the hockey stick divergence between the
empirical distributions of the output of a mechanism on neighboring dataset
using its dual formulation as the weighted accuracy of a classifier.
"""

import dataclasses
from typing import Tuple

from absl import logging
import numpy as np
import tensorflow as tf

from dp_auditorium import interfaces
from dp_auditorium.configs import privacy_property
from dp_auditorium.configs import property_tester_config
from dp_auditorium.testers import property_tester_utils


@dataclasses.dataclass(frozen=False)
class HockeyStickDivergenceTrainingOptions:
  """Training options for the HockeyStickDivergenceTester.

  Attributes:
    num_epochs: Number of epochs to run the training pipeline.
    learning_rate: Learning rate for Adam optimizer.
    batch_size: Batch size to use during training.
    failure_probability: Probability of returning a false positive. That is, the
      divergence suggests the mechanism is not private even though it is.
    verbose: Integer to pass to keras to decide the verbosity of the training
      process.
  """

  num_epochs: int
  learning_rate: float
  batch_size: int
  verbose: int


def make_default_hs_training_config() -> property_tester_config.TrainingConfig:
  return property_tester_config.TrainingConfig(
      training_epochs=2,
      optimizer_learning_rate=1e-2,
      batch_size=100,
      verbose=0,
  )


def make_default_hs_base_model() -> tf.keras.Model:
  return tf.keras.Sequential([
      tf.keras.layers.Dense(12, activation="relu"),
      tf.keras.layers.Dense(12, activation="relu"),
      tf.keras.layers.Dense(12, activation="relu"),
      tf.keras.layers.Dense(1),
  ])


def make_training_options_from_config(
    training_config: property_tester_config.TrainingConfig,
):
  return HockeyStickDivergenceTrainingOptions(
      num_epochs=training_config.training_epochs,
      learning_rate=training_config.optimizer_learning_rate,
      batch_size=training_config.batch_size,
      verbose=training_config.verbose,
  )


# Helper functions and classes for the HockeyStickDivergenceTester
def _get_accuracy_confidence_bound(
    n_samples: int, confidence: float = 0.95
) -> float:
  r"""Returns a confidence bound on the estimate of P(h(X) = y).

  Uses Hoeffding's inequality to estimate this using
  \frac{1}{n} \sum_{i=1}^n {h(X_i) = Y_i}.

  Args:
    n_samples: Number of samples used in the estimate.
    confidence: The level of confidence we want the estimate to have.

  Returns:
    The one-sided confidence error around the estimate.
  """
  delta = 1.0 - confidence
  return np.sqrt(np.log(1.0 / delta) / 2.0 / n_samples)


class HockeyStickPropertyTester(interfaces.PropertyTester):
  r"""Uses a model to estimate divergence between the outputs of a mechanism.

  Specifically, given two neighboring datasets D_0, D_1 and epsilon. Generates
  samples (X_i, Y_i) as follows Y_i \in {0, 1} where Y_i = 0
  w.p. 1/(1 + e^epsilon)and 1 otherwise. X_i ~ Mechansim(D_{Y_i}). The model
  tries to distinguish between "positive" and "negative" examples. A mechanism
  is (epsilon,delta) DP if and only if the accuracy of a classifier in this
  dataset is less than (e^epsilon + delta) / (1 + e^epsilon). The hockey stick
  divergence corresponds to \delta.

  Attributes:
    _base_model: A keras model that discriminates between samples generated by a
      mechanism ran on two different datasets. The base_model passed into this
      class must return logits.
    _epsilon: The epsilon in the (epsilon, delta) guarantee the mechanism is
      supposed to satisfy.
    _delta: The delta in the (epsilon,delta) guarantee the mechanism is supposed
      to satisfy.
    _has_called_fit: Boolean that verifies if model has been trained.
  """

  def __init__(
      self,
      config: property_tester_config.HockeyStickPropertyTesterConfig,
      base_model: tf.keras.Model,
  ):
    """Initializes the instance.

    Args:
      config: Configuration for initializing property tester.
      base_model: A keras model that discriminates between samples generated by
        a mechanism ran on two different datasets. The base_model passed into
        this class must return logits.
    """
    property_tester_utils.validate_approximate_dp_property(
        config.approximate_dp
    )
    self._base_model = base_model
    self._epsilon = config.approximate_dp.epsilon
    self._delta = config.approximate_dp.delta
    self._approximate_dp = config.approximate_dp
    self._has_called_fit = False
    self._training_options = make_training_options_from_config(
        config.training_config
    )
    self.initialize(self._training_options)

  @property
  def privacy_property(self) -> privacy_property.PrivacyProperty:
    """The privacy guarantee that the tester is being used to test for."""
    return privacy_property.PrivacyProperty(approximate_dp=self._approximate_dp)

  def initialize(self, training_options: HockeyStickDivergenceTrainingOptions):
    """Compiles internal model.

    Auxiliary function to use as a standalone tester while changing training
    options of the tester across different runs.

    Args:
      training_options: Training options for keras optimization.
    """

    if self._has_called_fit:
      self._base_model = tf.keras.models.clone_model(self._base_model)
    self._base_model.compile(
        optimizer=tf.keras.optimizers.Adam(training_options.learning_rate),
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
        metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.0),
    )
    self._training_options = training_options

  def _generate_inputs_to_model(
      self,
      samples1: np.ndarray,
      samples2: np.ndarray,
  ) -> Tuple[np.ndarray, np.ndarray]:
    """Generates inputs to keras classifiers.

    Generates a weighted sample for the input to a classifer.
    It takes the first 1/(e^epsilon + 1)samples from samples1 and labels
    them as 0, it uses the last e^epsilon/(e^epsilon + 1) samples from
    samples2 and labels them as 1.

    Args:
      samples1: First set of samples.
      samples2: Second set of samples.

    Returns:
      Features and labels as described above, where features correspond to the
      output of the mechanism.

    Raises:
      ValueError if the ranks of sample1 and sample 2 are not equal.
    """
    sample_cutoff_fraction = 1.0 / (np.exp(self._epsilon) + 1)

    if len(samples1.shape) != len(samples2.shape):
      raise ValueError(f"""Mechanism outputs on dataset 1 and dataset 2 should
          have the same rank got {samples1.shape} and
           {samples2.shape}""")
    if len(samples1.shape) == 1:
      samples1 = samples1.reshape(-1, 1)
      samples2 = samples2.reshape(-1, 1)
    samples1_final_ix = int(sample_cutoff_fraction * len(samples1))
    samples2_initial_ix = int(sample_cutoff_fraction * len(samples2))
    samples1 = samples1[0:samples1_final_ix, ...]
    samples2 = samples2[samples2_initial_ix:, ...]

    labels_1 = np.zeros((samples1.shape[0], 1))
    labels_2 = np.ones((samples2.shape[0], 1))
    features = np.concatenate([samples1, samples2], axis=0)
    labels = np.concatenate([labels_1, labels_2], axis=0)
    return features, labels

  def _fit(
      self,
      samples1: np.ndarray,
      samples2: np.ndarray,
      batch_size: int,
      epochs: int,
      verbose: int,
  ):
    """Fits the underlying model on the labeled output of a mechansim.

    Args:
      samples1: Samples from one distribution
      samples2: Samples from the other distribution
      batch_size: Batch size to use in the training process.
      epochs: Number of epochs to train for.
      verbose: Option passed to keras trainer.
    """
    self.initialize(self._training_options)
    features, labels = self._generate_inputs_to_model(samples1, samples2)
    self._base_model.fit(
        features,
        labels,
        shuffle=True,
        epochs=epochs,
        batch_size=batch_size,
        verbose=verbose,
    )
    self._has_called_fit = True

  def _get_accuracy_and_divergence_estimate(
      self,
      samples1,
      samples2,
      failure_probability: float,
  ) -> Tuple[float, float]:
    """Returns the accuracy of a trained classifier.

    Args:
      samples1: Samples from one distribution
      samples2: Samples from the other distribution
      failure_probability: Probability of having a false positive. I.e. the test
        suggests that this is a privacy violation when in reality it is not.

    Returns:
      The accuracy of the classifier (adjusted with confidence) and the
      associated hockey stick divergence (this corresponds to the delta of the
      mechanism).

    Raises:
      AttributeError if called before calling fit().
    """
    if not self._has_called_fit:
      raise AttributeError(
          "Estimator should be trained with fit() before getting accuracy"
      )
    features, labels = self._generate_inputs_to_model(samples1, samples2)

    accuracy = self._base_model.evaluate(features, labels, batch_size=1000)[1]
    test_sample_size = samples1.shape[0]
    accuracy -= _get_accuracy_confidence_bound(
        test_sample_size, 1 - failure_probability
    )
    hs_divergence = accuracy * (1 + np.exp(self._epsilon)) - np.exp(
        self._epsilon
    )
    return accuracy, hs_divergence

  def _estimate_discriminative_accuracy_and_hs_divergence_of_mechanism(
      self, samples_1, samples_2, failure_probability: float
  ) -> Tuple[float, float]:
    """End to end estimation of accuracy and divergence.

    Args:
      samples_1: Samples from one distribution
      samples_2: Samples from the other distribution
      failure_probability: The probability of the test asserting that the the
        accuracy estimated by the method is lower than the returned value.

    Returns:
      Accuracy and hockey stick divergence of the mechanism on datasets.
    """
    train_samples1, test_samples_1 = (
        property_tester_utils.split_train_test_samples(
            samples_1
        )
    )
    train_samples2, test_samples_2 = (
        property_tester_utils.split_train_test_samples(
            samples_2
        )
    )
    self._fit(
        train_samples1,
        train_samples2,
        epochs=self._training_options.num_epochs,
        batch_size=self._training_options.batch_size,
        verbose=self._training_options.verbose,
    )
    logging.info("Evaluating model")
    return self._get_accuracy_and_divergence_estimate(
        test_samples_1,
        test_samples_2,
        failure_probability=failure_probability,
    )

  def estimate_lower_bound(
      self,
      samples_1: np.ndarray,
      samples_2: np.ndarray,
      failure_probability: float,
  ) -> float:
    """Returns a lower bound on the hockey stick divergence between the samples.

    Args:
      samples_1: First set of samples.
      samples_2: Second set of samples
      failure_probability: The probability that the returned value is not in
        fact a lower bound on the divergence between the distributions that
        generated these samples.

    Returns:
      Estimated lower bound on the divergence between two distributions
      represented by samples_1 and samples_2.
    """
    accuracy, divergence = (
        self._estimate_discriminative_accuracy_and_hs_divergence_of_mechanism(
            samples_1, samples_2, failure_probability
        )
    )
    logging.info("Accuracy: %f, Divergence: %f", accuracy, divergence)
    return divergence

  def reject_property(self, lower_bound: float) -> bool:
    """Tests whether a mechanism is epsilon-delta private.

    Args:
      lower_bound: Divergence obtained from estimate_divergence

    Returns:
      True if the esitmated lower bound on the divergence is above the
      expected delta parameter of a mechanism.
    """
    return lower_bound > self._delta
